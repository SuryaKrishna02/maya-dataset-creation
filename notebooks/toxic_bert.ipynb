{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438a03da-0bea-4e13-a179-d49c79d51528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from detoxify import Detoxify\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1beb6c9a-dc98-4de8-b711-6642f672e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_json(\"../dataset/blip_laion_cc_sbu_558k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "742a0e12-0ebc-46e5-96a2-f12d3fdcd50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = []\n",
    "\n",
    "for index, row in data_df.iterrows():\n",
    "    conversations = row['conversations']\n",
    "    for conversation in row['conversations']:\n",
    "        if conversation['from'] == 'gpt':\n",
    "            gpt_messages.append(\n",
    "                {\n",
    "                    'id': row['id'],\n",
    "                    'message': conversation['value']\n",
    "                }\n",
    "            )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6683261-7149-4746-80bc-210ed8adc260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch):\n",
    "    \"\"\"\n",
    "    Process a batch of records through Detoxify model and add toxicity score.\n",
    "    \n",
    "    Parameters:\n",
    "    batch (list of dict): A list of dictionaries with 'id' and 'message'.\n",
    "    \n",
    "    Returns:\n",
    "    list of dict: A list of dictionaries with 'id', 'message', and 'toxicity_score'.\n",
    "    \"\"\"\n",
    "    messages = [entry['message'] for entry in batch]\n",
    "    ids = [entry['id'] for entry in batch]\n",
    "\n",
    "    # Get predictions\n",
    "    results = Detoxify('original').predict(messages)\n",
    "    \n",
    "    # Extract toxicity scores\n",
    "    toxicity_scores = results['toxicity']\n",
    "    \n",
    "    # Combine results with original data\n",
    "    processed_batch = [\n",
    "        {'id': id_, 'message': message, 'toxicity_score': toxicity_score}\n",
    "        for id_, message, toxicity_score in zip(ids, messages, toxicity_scores)\n",
    "    ]\n",
    "    \n",
    "    return processed_batch\n",
    "\n",
    "def process_dataset(data, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Process the entire dataset in batches with a progress bar and return a DataFrame with toxicity scores.\n",
    "    \n",
    "    Parameters:\n",
    "    data (list of dict): A list of dictionaries representing the dataset.\n",
    "    batch_size (int): Number of records to process in each batch.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with 'id', 'message', and 'toxicity_score' columns.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    total_batches = (len(data) + batch_size - 1) // batch_size  # Calculate total number of batches\n",
    "\n",
    "    # Process in batches with a progress bar\n",
    "    for i in tqdm(range(0, len(data), batch_size), total=total_batches, desc=\"Detecting Toxicity\"):\n",
    "        batch = data[i:i+batch_size]\n",
    "        processed_batch = process_batch(batch)\n",
    "        all_results.extend(processed_batch)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c18ff807-0325-46d3-973d-7d3a23342b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting Toxicity:   0%|                                                                      | 0/559 [00:00<?, ?it/s]D:\\Github\\maya-dataset-creation\\maya-venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Detecting Toxicity: 100%|██████████████████████████████████████████████████████████| 559/559 [7:57:20<00:00, 51.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process the dataset\n",
    "result_df = process_dataset(gpt_messages, batch_size=1000)\n",
    "\n",
    "# Save the result to a JSON file\n",
    "result_df.to_json('../dataset/toxicity_detection.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f29e71-7cd2-4192-a956-58f634a62167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
